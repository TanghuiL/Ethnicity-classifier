{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethnicity classifier with a Conv Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "__author__ = \"Dr. Joann H. Tang & Dr. Rahul Remanan\"\n",
    "__copyright__ = \"Copyright 2018\"\n",
    "__email__ = \"eagtang2007@gmail.com,rahul@remanan.net\"\n",
    "__status__ = \"Prototype\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras.utils\n",
    "from keras.models import Sequential\n",
    "#Core layers\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "#CNN layers\n",
    "from keras.layers import SeparableConv2D, Conv2D, MaxPooling2D\n",
    "\n",
    "from sklearn.utils import shuffle \n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "from PIL import Image\n",
    "import glob\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the directory path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = \"./Pooled/\"\n",
    "save_path = \"./model_weights.h5\"\n",
    "load_trained_model = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function for extracting pixel values from images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_pixel_values(prefix,y_label,width,height):\n",
    "    \"\"\"\n",
    "    Resize image to the given width and height. \n",
    "    Then extract pixel values from the resized images. \n",
    "    \n",
    "    Arguments:\n",
    "    prefix -- The images for each class were labeled with a specific prefix\n",
    "    y_label -- The number assigned to represent a specific ethnicity group\n",
    "    width, height -- the width and height of the target image size\n",
    "    \n",
    "    Returns:\n",
    "    X -- pixel values of images, \n",
    "         numpy array of shape(number_of_images,width,height,3)\n",
    "    Y -- the ethnicity group label of images, \n",
    "         numpy array of shape(number_of_images)\n",
    "    \"\"\"\n",
    "    ims = glob.glob(path + prefix + \"*.jpg\")\n",
    "    \n",
    "    Y = np.zeros((np.size(ims),1))\n",
    "    Y[:] = int(y_label)\n",
    "    X = np.zeros((np.size(ims),width,height,3)) \n",
    "    for i, im in enumerate(ims):\n",
    "        img = Image.open(im)\n",
    "        img_resized = img.resize([width,height],Image.ANTIALIAS) \n",
    "        X[i] = np.asarray(img_resized)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function for splitting dataset into training set and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_dataset(X,Y,percentage):\n",
    "    \"\"\"\n",
    "    Randomly select a defined percentge of the dataset as train set \n",
    "    and divide the rest for validation set and test set \n",
    "   \n",
    "    Arguments:\n",
    "    X -- numpy array of feature data, here, they are the pixel values of images\n",
    "    Y -- numpy array of label data\n",
    "    percentage -- the percentage of the total dataset that assigned as training set\n",
    "    \n",
    "    Returns:\n",
    "    x_train -- pixel values of images in the training set\n",
    "    y_train -- labels of images in the training set\n",
    "    x_val -- pixel values of images in the validation set\n",
    "    y_val -- labels of images in the validation set\n",
    "    x_test -- pixel values of images in the test set\n",
    "    y_test -- labels of images in the test set\n",
    "\n",
    "    \"\"\"\n",
    "    idx_train = np.random.randint(np.size(Y), size=round(np.size(Y)*percentage))\n",
    "    idx_val = np.random.randint(np.size(Y), size=round(np.size(Y)*0.5*(1-percentage)))\n",
    "    idx_test = np.random.randint(np.size(Y), size=round(np.size(Y)*0.5*(1-percentage)))\n",
    "    x_train = X[idx_train,:]\n",
    "    y_train = Y[idx_train]\n",
    "    x_val = X[idx_val,:]\n",
    "    y_val = Y[idx_val]\n",
    "    x_test = X[idx_test,:]\n",
    "    y_test = Y[idx_test]\n",
    "    return x_train, y_train, x_val, y_val, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating dataset\n",
    "This project uses the United States Census Bureau classification of ethinicites in the US.\n",
    "\n",
    "Currently the dataset is limited to four ethnic groups in the US:\n",
    "1) Asian American\n",
    "\n",
    "2) Black and African American\n",
    "\n",
    "3) Caucasian/White and Euroopean American\n",
    "\n",
    "4) Hispanic and Latino American\n",
    "\n",
    "The dataset is missing information on:\n",
    "1) Native American and Alaska Native\n",
    "\n",
    "2) Native Hawaiian and other Pacific Islander"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ethnicity group -- Asian American"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, Y = extract_pixel_values(\"A\",0,128,128) \n",
    "x1_train,y1_train,x1_val,y1_val,x1_test,y1_test = split_dataset(X,Y,0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ethnicity group -- Black and African American"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, Y = extract_pixel_values(\"B\",1,128,128)                          \n",
    "x2_train,y2_train,x2_val,y2_val,x2_test,y2_test = split_dataset(X,Y,0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ethnicity group -- Caucasian/White and European American"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, Y = extract_pixel_values(\"C\",2,128,128)                          \n",
    "x3_train,y3_train,x3_val,y3_val,x3_test,y3_test = split_dataset(X,Y,0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ethnicity group -- Hispanic and Latino Americans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, Y = extract_pixel_values(\"H\",3,128,128)                          \n",
    "x4_train,y4_train,x4_val,y4_val,x4_test,y4_test = split_dataset(X,Y,0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create training dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = min(len(y1_train),len(y2_train),len(y3_train),len(y4_train))\n",
    "x_train = np.concatenate((x1_train[0:n], x2_train[0:n], x3_train[0:n], x4_train[0:n]), axis=0)\n",
    "y_train = np.concatenate((y1_train[0:n], y2_train[0:n], y3_train[0:n], y4_train[0:n]), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create validation dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = min(len(y1_val),len(y2_val),len(y3_val),len(y4_val))\n",
    "x_val = np.concatenate((x1_val, x2_val, x3_val, x4_val), axis=0)\n",
    "y_val = np.concatenate((y1_val, y2_val, y3_val, y4_val), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Randomize the training and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, y_train = shuffle(x_train, y_train, random_state=1024)\n",
    "x_val, y_val = shuffle(x_val, y_val, random_state=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensuring data casting to the right data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = (x_train).astype('float32')\n",
    "x_val = (x_val).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train /= 255\n",
    "x_val /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up parameters for the classifier¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_classes = 4\n",
    "epochs = 5\n",
    "dropout = 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert class vectors to binary class matrices¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(np.asarray(y_train), num_classes)\n",
    "y_val = keras.utils.to_categorical(np.asarray(y_val), num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a convolutional neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Declare a sequential model\n",
    "model = Sequential()\n",
    "#CNN input layer \n",
    "model.add(SeparableConv2D(32, kernel_size =(3,3), \n",
    "                 activation='relu', \n",
    "                 depth_multiplier = 3,\n",
    "                 padding = 'same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "\n",
    "#Add hidden layers to the model \n",
    "model.add(Conv2D(32,(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Conv2D(64,(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Conv2D(64,(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(dropout))\n",
    "\n",
    "#Fully connected Dense layers \n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "import io, json\n",
    "with io.open('model.config', 'w', encoding='utf-8') as f:\n",
    "  f.write(json.dumps(model_json, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if load_trained_model:\n",
    "    model.load_weights(save_path, by_name = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run evaluation on the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
